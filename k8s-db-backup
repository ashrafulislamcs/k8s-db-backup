#!/usr/bin/env python3

from io import BytesIO
import boto3
import os, datetime, shutil, gzip
from subprocess import Popen, run, PIPE

S3_PROVIDER=os.getenv('S3_PROVIDER', '')
BACKUP_PATH_PREFIX=os.getenv('BACKUP_PATH_PREFIX', 'k8s-db-backup')

def get_provider_cfg(provider):
    if provider.lower() not in ['oci', 'aws']:
        raise Exception('Provider {} not supported. Supported providers are: aws and oci'.format(provider))
    
    cfg = {
        'provider': provider.lower(),
        'access_key': os.getenv('{}_ACCESS_KEY'.format(provider.upper()), ''),
        'secret_key': os.getenv('{}_SECRET_KEY'.format(provider.upper()), ''),
        'region': os.getenv('{}_REGION'.format(provider.upper()), ''),
    }
    # Set extra-variable for OCI provider
    if provider.lower() == 'oci':
        oci_endpoint = "https://{tenency_namespace}.compat.objectstorage.{region}.oraclecloud.com".format(
            tenency_namespace=os.getenv('{}_NAMESPACE'.format(provider.upper()), ''), 
            region=os.getenv('{}_REGION'.format(provider.upper()), '')
        )
        cfg['endpoint'] = oci_endpoint
        cfg['namespace'] = os.getenv('{}_NAMESPACE'.format(provider.upper()), ''),
    return cfg

def get_boto_resource(provider_cfg):
    if provider_cfg['provider'] not in ['oci', 'aws']:
        raise Exception('Provider {} not supported. Supported providers are: aws and oci'.format(provider_cfg['provider']))

    s3 = boto3.resource(
        's3',
        region_name=provider_cfg['region'],
        aws_secret_access_key=provider_cfg['secret_key'], 
        aws_access_key_id=provider_cfg['access_key'],
        endpoint_url=None if provider_cfg['provider'] == 'aws' else provider_cfg['endpoint'],
    )
    return s3

def upload_file(provider_cfg, bucket_name, src_filename, dst_filename, extra_args=None):
    s3 = get_boto_resource(provider_cfg=provider_cfg)
    s3.meta.client.upload_file(src_filename, bucket_name, dst_filename, extra_args)

def upload_object(provider_cfg, bucket_name, src_object, dst_filename, extra_args=None):
    s3 = get_boto_resource(provider_cfg=provider_cfg)
    s3.meta.client.upload_fileobj(src_object, bucket_name, dst_filename, extra_args)

def download_file(provider_cfg, bucket_name, src_filename, local_filename):
    s3 = get_boto_resource(provider_cfg=provider_cfg)
    bucket = s3.Bucket(bucket_name)
    bucket.download_file(src_filename, local_filename)
    
def download_object(provider_cfg, bucket_name, src_object):
    s3 = get_boto_resource(provider_cfg=provider_cfg)
    bucket = s3.Bucket(bucket_name)
    file_object = BytesIO()

    bucket.download_fileobj(src_object, file_object)
    file_object.seek(0)

    return file_object.getvalue()

def compress_and_upload(backup_filename, backup_filename_gz, backup_filename_gz_remote):
    with open(backup_filename, 'rb') as f_in:
        with gzip.open(backup_filename_gz, 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)
    
    os.remove(backup_filename)
    
    provider_cfg = get_provider_cfg(S3_PROVIDER)
    
    extra_args = {}
    if S3_PROVIDER.lower() == 'aws':
        extra_args = {'StorageClass': 'STANDARD'}
    
    bucket_name = os.getenv('{}_BUCKET_NAME'.format(S3_PROVIDER.upper()), '')
    upload_file(
        provider_cfg=provider_cfg,
        bucket_name=bucket_name,
        src_filename=backup_filename_gz, 
        dst_filename=backup_filename_gz_remote, 
        extra_args=extra_args
    )

def get_db_cfg():
    db_cfg = {
        'user': os.getenv('MYSQL_USER', ''),
        'password': os.getenv('MYSQL_PASSWORD', ''),
        'database': os.getenv('MYSQL_DATABASE', ''),
        'host': os.getenv('MYSQL_HOST', ''),
        'backup_path': os.getenv('BACKUP_PATH', '/backup'),
    }
    return db_cfg

def backup():
    db_cfg = get_db_cfg()
    backup_strategy = os.getenv('BACKUP_STRATEGY', 'SINGLE_FILE_DUMP')
    if backup_strategy == 'SINGLE_FILE_DUMP':
        backup_single_file_dump(db_cfg)
    else:
        backup_split_by_table(db_cfg)

def backup_single_file_dump(db_cfg):
    today = datetime.datetime.now().strftime('%Y-%m-%d')
    basedir = '{}/{}'.format(db_cfg['backup_path'], BACKUP_PATH_PREFIX)
    database = db_cfg['database']
    
    # Cleanup previous jobs
    try:
        shutil.rmtree(basedir)
    except Exception as e:
        print('WARN: {} not found. Create a new empty dir'.format(basedir))
    
    # Create base dir
    os.makedirs(basedir)

    backup_filename = '{}/{}-db-dump-{}.sql'.format(basedir, database, today)
    backup_filename_gz = '{}/{}-db-dump-{}.sql.gz'.format(basedir, database, today)
    backup_filename_gz_remote = '{}/{}-db-dump-{}.sql.gz'.format(BACKUP_PATH_PREFIX, database, today)
    
    filename_obj = open(backup_filename, "w")

    p = Popen(
        [
            "/usr/bin/mysqldump", "--no-tablespaces", "--skip-lock-tables", 
            "-u", db_cfg['user'] , '-p{}'.format(db_cfg['password']), '-h', db_cfg['host'], db_cfg['database']
        ],
        stdout=filename_obj, 
        stderr=PIPE
    )
    ret_code = p.wait()
    filename_obj.flush()
    filename_obj.close()

    if ret_code == 0:
        compress_and_upload(
            backup_filename=backup_filename,
            backup_filename_gz=backup_filename_gz,
            backup_filename_gz_remote=backup_filename_gz_remote,
        )    
    shutil.rmtree(basedir)

def backup_split_by_table(db_cfg):
    today = datetime.datetime.now().strftime('%Y-%m-%d')
    basedir = '{}/{}/{}'.format(db_cfg['backup_path'], BACKUP_PATH_PREFIX, today)
    database = db_cfg['database']
    
    # Cleanup previous jobs
    try:
        shutil.rmtree(basedir)
    except Exception as e:
        print('WARN: {} not found. Create a new empty dir'.format(basedir))
    
    # Create base dir
    os.makedirs(basedir)

    tables = run(
        [
            "mysql", "-u", db_cfg['user'] , '-p{}'.format(db_cfg['password']), '-h', 
            db_cfg['host'], db_cfg['database'], '-Ns', '-e', 'show tables;'
        ], 
        capture_output=True, 
        text=True
    )

    if tables.stdout:
        for table in tables.stdout.split('\n'):
            if not table:
                continue

            backup_filename = '{}/{}-{}.sql'.format(basedir, database, table)
            backup_filename_gz = '{}/{}-{}.sql.gz'.format(basedir, database, table)
            backup_filename_gz_remote = '{}/{}/{}-{}.sql.gz'.format(BACKUP_PATH_PREFIX, today, database, table)
            
            filename_obj = open(backup_filename, "w")

            p = Popen(
                [
                    "/usr/bin/mysqldump", "--no-tablespaces", "--skip-lock-tables", 
                    "-u", db_cfg['user'] , '-p{}'.format(db_cfg['password']), '-h', db_cfg['host'], db_cfg['database'], table
                ],
                stdout=filename_obj, 
                stderr=PIPE
            )
            ret_code = p.wait()
            filename_obj.flush()
            filename_obj.close()
            if ret_code == 0:
                compress_and_upload(
                    backup_filename=backup_filename,
                    backup_filename_gz=backup_filename_gz,
                    backup_filename_gz_remote=backup_filename_gz_remote,
                )
    shutil.rmtree(basedir)

if __name__ == '__main__':
    backup()